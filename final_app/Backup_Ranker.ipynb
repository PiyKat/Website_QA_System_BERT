{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "\n",
    "class DocumentRanker():\n",
    "    \n",
    "    def __init__(self, query, folderPath):\n",
    "        \n",
    "        self.documentFolderPath = folderPath\n",
    "        print(\"folderPath  : \", folderPath)\n",
    "        self.query = self.convertQuery(query)\n",
    "        self.folderDocumentContent = self.loadDocuments()  # Store the content of all the files in dictionary\n",
    "        self.bm25Model = self.createBM25Model()\n",
    "    \n",
    "    \n",
    "    def convertQuery(self,query):\n",
    "        \n",
    "        nlp = spacy.load(\"en_core_web_sm\",disable = ['ner', 'parser', 'textcat'])\n",
    "        doc = nlp(query.lower())\n",
    "        return doc\n",
    "        \n",
    "    def loadDocuments(self):\n",
    "        '''\n",
    "        Load documents from the invest_data folder into a dictionary\n",
    "        '''\n",
    "        documentTuple = []\n",
    "        nounList = self.getNouns()\n",
    "        nlp = spacy.load(\"en_core_web_sm\",disable = ['ner', 'parser', 'textcat'])\n",
    "        \n",
    "        for root, dirs, files in os.walk(self.documentFolderPath):\n",
    "    \n",
    "            for file in files:\n",
    "                if file.endswith('txt'):\n",
    "                    with open(os.path.abspath(os.path.join(root, file)), 'r' , encoding=\"utf8\" ) as f:\n",
    "                        content = f.read()\n",
    "                        content = content.lower()\n",
    "                        doc = nlp(content)\n",
    "                        tokenizedContent = [tokens.text for tokens in doc] #if tokens.text not in ['a','an','the']]  \n",
    "                        documentTuple.append((file,tokenizedContent))\n",
    "                            \n",
    "        return documentTuple\n",
    "    \n",
    "    def getNouns(self):\n",
    "        \n",
    "        #nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        '''\n",
    "        spacy_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "        \n",
    "        querySplit = self.query.lower().split()\n",
    "        \n",
    "        querySplit = [x for x in querySplit if x not in spacy_stopwords]\n",
    "        \n",
    "        print(querySplit)\n",
    "        \n",
    "        return querySplit\n",
    "        \n",
    "        '''\n",
    "        nounList = []\n",
    "        for token in self.query:\n",
    "            #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" or token.pos_ ==\"ADJ\":\n",
    "\n",
    "                nounList.append(token.text)\n",
    "            \n",
    "        print(\"noun list : \", nounList)        \n",
    "        return nounList\n",
    "    \n",
    "        \n",
    "    def createBM25Model(self):\n",
    "        '''\n",
    "        Return a created BM25 model.\n",
    "        '''\n",
    "        return gensim.summarization.bm25.BM25([x[1] for x in self.folderDocumentContent])\n",
    "    \n",
    "    def rankDocuments(self):\n",
    "        '''\n",
    "        Rank the documents in the corpus wrt the query. \n",
    "        '''\n",
    "        query = [tokens.text for tokens in self.query]# if tokens.text not in [\"a\",\"an\",\"the\"]]\n",
    "        scores = self.bm25Model.get_scores(query)\n",
    "        documentScores =  [(self.folderDocumentContent[i][0],scores[i]) for i in range(len(scores))]\n",
    "        return documentScores\n",
    "    \n",
    "    def returnTopK(self,k=5):\n",
    "        '''\n",
    "        Return top k ranked documents wrt given user query\n",
    "        '''\n",
    "        \n",
    "        scoreDict = self.rankDocuments()\n",
    "        return sorted(scoreDict , key = lambda x: x[1], reverse = True)[:k]\n",
    "    \n",
    "    def returnTopDocuments(self):\n",
    "        '''\n",
    "        Return top documents wrt given user query. This uses a different approach from our \n",
    "        previous function. We return all documents with score a standard deviation above the\n",
    "        mean score of our corpus\n",
    "        '''\n",
    "        \n",
    "        scoreTup = self.rankDocuments()\n",
    "        #scoreDict = dict(scoreTup)\n",
    "        scores = np.array([x[1] for x in scoreTup])\n",
    "        #scores = np.array(list(scoreDict.values()))\n",
    "        meanScore = np.mean(scores)\n",
    "        sdScore = np.std(scores)\n",
    "        maxThreshold = meanScore + sdScore\n",
    "        topScoresDict = [x for x in scoreTup if x[1] > maxThreshold]\n",
    "        topDocumentScores = sorted(topScoresDict, key = lambda x: x[1], reverse = True)\n",
    "        \n",
    "        return topDocumentScores\n",
    "    \n",
    "    def returnTopDocumentsData(self):\n",
    "        '''\n",
    "        Return data from the top documents retrieved by returnTopDocuments\n",
    "        '''\n",
    "        #query = self.query\n",
    "        topDocumentScores = self.returnTopDocuments()\n",
    "        print(\"topDocumentScores  : \", topDocumentScores)\n",
    "        topDocumentNames = [x[0] for x in topDocumentScores]\n",
    "        #print(topDocumentNames)\n",
    "        \n",
    "        topDocumentText = [x for x in self.folderDocumentContent if x[0] in topDocumentNames]\n",
    "        return topDocumentText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 \n",
    "\n",
    "class PassageRanker():\n",
    "    \n",
    "    def __init__(self, query, topDocumentContent):\n",
    "        \n",
    "        self.query = self.convertQuery(query)\n",
    "        self.topDocumentContent = topDocumentContent\n",
    "        #self.BM25Model = self.createBM25Model()\n",
    "    \n",
    "    def convertQuery(self,query):\n",
    "        \n",
    "        nlp = spacy.load(\"en_core_web_sm\",disable = ['ner', 'parser', 'textcat'])\n",
    "        doc = nlp(query.lower())\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def createBM25Model(self,documentContent):\n",
    "        '''\n",
    "        Return a created BM25 model.\n",
    "        '''\n",
    "        #return gensim.summarization.bm25.BM25([x[1] for x in self.topDocumentContent])\n",
    "        return gensim.summarization.bm25.BM25(documentContent)\n",
    "    \n",
    "    \n",
    "    def getNouns(self):\n",
    "        \n",
    "        #nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        '''\n",
    "        spacy_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "        \n",
    "        querySplit = self.query.lower().split()\n",
    "        \n",
    "        querySplit = [x for x in querySplit if x not in spacy_stopwords]\n",
    "        \n",
    "        print(querySplit)\n",
    "        \n",
    "        return querySplit\n",
    "        \n",
    "        '''\n",
    "        nounList = []\n",
    "        for token in self.query:\n",
    "            #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" or token.pos_ == \"ADJ\":\n",
    "                nounList.append(token.text)\n",
    "            \n",
    "        #print(\"noun list : \", nounList)        \n",
    "        return nounList\n",
    "    \n",
    "    def checkNounMatch(self,paragraph,nounList):\n",
    "        '''\n",
    "        A function to see if each noun is matching for a paragraph\n",
    "        '''\n",
    "        '''\n",
    "        nounBool = True\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        #\n",
    "        #stemmedPara = [stemmer.stem(token) for token in paragraph]\n",
    "        paragraphContent = \" \".join(paragraph)\n",
    "        for nouns in nounList: #nounsLemma:\n",
    "            \n",
    "            #nounLemma = [tokens.lemma_ for tokens in nounDoc]\n",
    "            \n",
    "            if stemmer.stem(nouns) in paragraphContent:#paragraphContent:\n",
    "                nounBool = True\n",
    "            else:\n",
    "                nounBool = False\n",
    "                break\n",
    "        \n",
    "        return nounBool\n",
    "        \n",
    "        '''\n",
    "        nounBool = True\n",
    "        \n",
    "        #nlp = spacy.load(\"en_core_web_sm\",disable=['ner', 'parser', 'textcat'])\n",
    "        \n",
    "        for nouns in nounList:\n",
    "            \n",
    "            #nounDoc = nlp(nouns)\n",
    "            #nounLemma = [tokens.lemma_ for tokens in nounDoc]\n",
    "            paragraphContent = \" \".join(paragraph)\n",
    "            #paragraphDoc = nlp(paragraphContent)\n",
    "            #paragraphLemma = [tokens.lemma_ for tokens in paragraphDoc]\n",
    "            \n",
    "            if nouns in paragraphContent:\n",
    "                nounBool = True\n",
    "            else:\n",
    "                nounBool = False\n",
    "                break\n",
    "        \n",
    "        return nounBool\n",
    "        \n",
    "        '''\n",
    "        nounBool = True\n",
    "        \n",
    "        #nlp = spacy.load(\"en_core_web_sm\",disable=['ner', 'parser', 'textcat'])\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        paragraphContent = \" \".join(paragraph)\n",
    "        \n",
    "        for nouns in nounList:\n",
    "            \n",
    "            if stemmer.stem(nouns) in paragraphContent:\n",
    "                nounBool = True\n",
    "            else:\n",
    "                nounBool = False\n",
    "                break\n",
    "        \n",
    "        return nounBool\n",
    "        '''        \n",
    "            \n",
    "    def returnParagraphList(self,content):\n",
    "        '''\n",
    "        Return pargaraph in the form of list of lists to be fed to the BM25 Model\n",
    "        '''\n",
    "        paragraphList = []\n",
    "        sepCounter = 0\n",
    "        nounList = self.getNouns()\n",
    "        \n",
    "        for i,term in enumerate(content):\n",
    "            \n",
    "            if term == \"--------------------------\":\n",
    "                paragraph = content[sepCounter:i] # \" \".join() <-\n",
    "                if self.checkNounMatch(paragraph,nounList):\n",
    "                #if all(nouns in paragraph for nouns in nounList):\n",
    "                    paragraphList.append(paragraph)\n",
    "                \n",
    "                sepCounter = i+1\n",
    "        \n",
    "        return paragraphList\n",
    "    \n",
    "    \n",
    "    def returnParagraphScores(self,bm25Model,query):\n",
    "        '''\n",
    "        Return the score of a paragraph given a model.\n",
    "        '''\n",
    "        return bm25Model.get_scores(query)\n",
    "        \n",
    "    \n",
    "    def returnTopPassages(self, k=10):\n",
    "        '''\n",
    "        Rank each paragraph from the document and return the top 10 passages from the collection of documents\n",
    "        '''\n",
    "        query = [tokens.text for tokens in self.query]\n",
    "        \n",
    "        #documentParagraphScores = [(document[0],self.rankPassages(document[1])) for document in topDocumentContent]\n",
    "        paragraphScoreTup = []\n",
    "        paragraphLoL = []\n",
    "        for documents in self.topDocumentContent:\n",
    "            #print(documents)\n",
    "            paragraphLoL.extend(self.returnParagraphList(documents[1]))\n",
    "        \n",
    "        \n",
    "        print(len(paragraphLoL))\n",
    "        #passageBM25Model = self.createBM25Model(paragraphLoL)\n",
    "        \n",
    "        #for documents in self.topDocumentContent:\n",
    "        #paragraphScores = self.returnParagraphScores(passageBM25Model, query)\n",
    "        for i,paragraph in enumerate(paragraphLoL):\n",
    "                \n",
    "            #paragraphScoreTup.append((paragraph,paragraphScores[i]))\n",
    "            paragraphScoreTup.append((paragraph,1))\n",
    "        \n",
    "        print(\" length of paragraphScoreTup : \", len(paragraphScoreTup))\n",
    "        \n",
    "        self.topParagraphScores = sorted(paragraphScoreTup, key = lambda x: x[1], reverse = True)\n",
    "        \n",
    "        return self.topParagraphScores#[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bertQAModel():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.tokenizerModel,self.bertQAModel = self.__initializeModel()\n",
    "    \n",
    "    def __initializeModel(self):\n",
    "        '''\n",
    "        Initialize the Beret Tokenizer and the QA model. Note that this is currently compatible with\n",
    "        transformers module, NOT pytorch/tensorflow\n",
    "        '''\n",
    "        with open(\"./models/bertTokenizer.pkl\",\"rb\") as f:\n",
    "            tokenizerModel = pickle.load(f)\n",
    "        \n",
    "        with open(\"./models/bertQAModel.pkl\",\"rb\") as f:\n",
    "            bertQAModel = pickle.load(f)\n",
    "        \n",
    "        return tokenizerModel, bertQAModel\n",
    "    \n",
    "    def stringProcess(self,answer):\n",
    "        answerSplit = answer.split(\" ##\")\n",
    "        return \"\".join(answerSplit)\n",
    "    \n",
    "    def preprocessQuery(self,query):\n",
    "        \n",
    "        queryNLP = spacy.load(\"en_core_web_sm\",disable = ['ner', 'parser', 'textcat'])\n",
    "        doc = queryNLP(query)\n",
    "        queryTokens = [tokens.text for tokens in doc if tokens.text not in [\"a\",\"an\",\"the\"]]\n",
    "        return \" \".join(queryTokens)\n",
    "    \n",
    "    def predict(self,text,question):\n",
    "        '''\n",
    "        Predict the answer given a passage and a question.\n",
    "        '''\n",
    "        question = self.preprocessQuery(question)\n",
    "        input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\n",
    "        #print(\"INPUT_TEXT : \")\n",
    "        #print(input_text)\n",
    "        \n",
    "        input_ids = self.tokenizerModel.encode(input_text)\n",
    "        #print(\"TOKENIZED TEXT : \")\n",
    "        #print(input_ids)\n",
    "        token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n",
    "        start_scores, end_scores = self.bertQAModel(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
    "        \n",
    "        all_tokens = self.tokenizerModel.convert_ids_to_tokens(input_ids)\n",
    "        answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
    "        startScoreInd,endScoreInd = torch.argmax(start_scores),torch.argmax(end_scores)\n",
    "        startScoresVec = start_scores.detach().numpy().flatten()\n",
    "        endScoresVec = end_scores.detach().numpy().flatten()\n",
    "        \n",
    "        startScoreMax, endScoreMax = startScoresVec[startScoreInd] , endScoresVec[endScoreInd]\n",
    "        #avgScore = float(np.absolute(startScoreMax) + np.absolute(endScoreMax))\n",
    "        #avgScore = float(startScoreMax + endScoreMax)\n",
    "        return (self.stringProcess(answer),startScoreMax)\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    '''\n",
    "    obj = bertQAModel()\n",
    "    obj.predict(\"Narendar Modi said that there is a lack of good employment opportunities in India\",\"What did Narendar Modi say?\")\n",
    "    '''\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
