 Standard Deviation vs. Variance . Variance is derived by taking the mean of the data points, subtracting the mean from each data point individually, squaring each of these results and then taking another mean of these squares. Standard deviation is the square root of the variance. The variance helps determine the data's spread size when compared to the mean value. As the variance gets bigger, more variation in data values occurs, and there may be a larger gap between one data value and another. If the data values are all close together, the variance will be smaller. This is more difficult to grasp than are standard deviations, however, because variances represent a squared result that may not be meaningfully expressed on the same graph as the original dataset. Standard deviations are usually easier to picture and apply. The standard deviation is expressed in the same unit of measurement as the data, which isn't necessarily the case with the variance. Using the standard deviation, statisticians may determine if the data has a normal curve or other mathematical relationship. If the data behaves in a normal curve, then 68% of the data points will fall within one standard deviation of the average, or mean data point. Bigger variances cause more data points to fall outside the standard deviation. Smaller variances result in more data that is close to average.