{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.summarization.bm25 \n",
    "import gensim\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import spacy\n",
    "import torch\n",
    "import transformers\n",
    "import pickle\n",
    "import numpy as np\n",
    "from nltk import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import urllib.request\n",
    "import re\n",
    "import timeit\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "class WebsiteData():\n",
    "    \n",
    "    def __init__(self, website,depth, pathName):#,website,parser = 'lxml'):\n",
    "        '''\n",
    "        Constructor of the class. We let the user choose the parser type, but keep it default to the fastest parser.\n",
    "        We are keeping the bsObject private because the end user should NOT get access to any HTML information.\n",
    "        '''\n",
    "        self.website = website\n",
    "        self.pageDict = {}\n",
    "        self.depth = depth\n",
    "        self.pathName = pathName\n",
    "        '''\n",
    "        try:\n",
    "            sauce = urllib.request.urlopen(self.website)\n",
    "            self.__bsObject = bs4.BeautifulSoup(sauce,parser)\n",
    "        except:\n",
    "            print(\"Website not accessible : \" ,self.website)\n",
    "            self.__bsObject = None\n",
    "            return None\n",
    "        '''\n",
    "            \n",
    "    def parseBaseWebsite(self, parser = 'lxml'):\n",
    "        try:\n",
    "            sauce = urllib.request.urlopen(self.website)\n",
    "            self.__bsObject = bs4.BeautifulSoup(sauce,parser)\n",
    "            return 1\n",
    "        except Exception as e:\n",
    "            print(\"Website not accessible. Exception :  \" ,e)\n",
    "            self.__bsObject = None\n",
    "            return None\n",
    "    \n",
    "    def __getBaseURL(self):\n",
    "        '''\n",
    "        Return the base url of the website being parsed\n",
    "        '''\n",
    "        count = 0\n",
    "        base1 = []\n",
    "        base = ''\n",
    "        if re.findall('http[s]?', self.website):\n",
    "            for i in self.website:\n",
    "                if i == '/':\n",
    "                    count= count+1\n",
    "                if count == 3:\n",
    "                    break\n",
    "                base1.append(i)\n",
    "        \n",
    "            base = base.join(base1)\n",
    "        \n",
    "        return base\n",
    "    \n",
    "    def displayHtml(self):\n",
    "        '''\n",
    "        Display website in HTML Structure \n",
    "        '''\n",
    "        print(self.__bsObject.prettify())\n",
    "    \n",
    "    def retrieveHrefs(self):\n",
    "        '''\n",
    "        Retrieve links from the HREF Tags in the webpage \n",
    "        '''\n",
    "        self.__linksList = []\n",
    "        baseUrl = self.__getBaseURL()\n",
    "        print(\"base url : \", baseUrl)\n",
    "        \n",
    "       \n",
    "        bodyContent = self.__bsObject.find(\"div\", attrs = {'id': 'mntl-sc-page_1-0'})\n",
    "        \n",
    "        \n",
    "        for link in bodyContent.find_all('a'):\n",
    "            #print(link)\n",
    "            suffixURL = link.get('href')\n",
    "            #print(type(suffixURL))\n",
    "            print(suffixURL)\n",
    "            \n",
    "            if (suffixURL is not None) and ('www.investopedia.com' in suffixURL) and (suffixURL.endswith('.asp'))  :\n",
    "                self.__linksList.append(str(suffixURL))\n",
    "                \n",
    "         \n",
    "        return self.__linksList\n",
    "    \n",
    "    def saveWebsiteData(self, levels):\n",
    "        '''\n",
    "        Save the data from the website in a txt file. The method creates a file with the topic of the link as the file name.\n",
    "        The code will be extended to store the data from all the links in the linksList in the future.\n",
    "        '''\n",
    "        pathName = self.pathName\n",
    "        if levels == 0 :\n",
    "            pathName = pathName\n",
    "        else:\n",
    "            for counter in range(levels):\n",
    "                pathName = pathName +  \"/level\" + str(counter+1 )\n",
    "                \n",
    "        if not os.path.exists(os.path.join(os.getcwd(),pathName)):\n",
    "            os.mkdir(os.path.join(os.getcwd(),pathName ))\n",
    "                \n",
    "\n",
    "        #headerContent = self.__bsObject.find_all(\"span\", attrs = {'class': 'mntl-sc-block-heading__text'})\n",
    "        bodyContent = self.__bsObject.find(\"div\", attrs = {'id': 'mntl-sc-page_1-0'})\n",
    "        paragraphs = bodyContent.find_all('p')\n",
    "        #headers = headerContent.find_all('')\n",
    "\n",
    "        print(\"writing data\")        \n",
    "        file_name = os.path.join(os.path.join(os.getcwd(), pathName), (self.website).split(\"/\")[-1] + \".txt\")\n",
    "        print(file_name)\n",
    "        with open(file_name ,\"w+\", encoding='utf-8') as f:\n",
    "            maxParaLen = 0\n",
    "            sumParaLen = 0\n",
    "            nlp = spacy.load(\"en_core_web_sm\",disable=['ner', 'parser', 'textcat'])\n",
    "            totalParaLen = len(paragraphs)\n",
    "            for paragraph in paragraphs:\n",
    "                print(\"paragraph   :  \", paragraph)\n",
    "                if paragraph.find_parent('div').find_previous('h3') is not None:\n",
    "                    heading = paragraph.find_parent('div').find_previous('h3')\n",
    "                    if heading.find('span') is not None:\n",
    "                        \n",
    "                        heading_text = heading.find('span').text\n",
    "                    else:\n",
    "                        heading_text = heading.text\n",
    "                else:\n",
    "                    heading = paragraph.find_parent('div').find_previous('h1')\n",
    "                    heading_text = heading.text\n",
    "                    \n",
    "                 \n",
    "                paraText = heading_text+ \". \" + paragraph.getText()\n",
    "                paraTextDoc = nlp(paraText)\n",
    "                paraTextDocList = [tokens.text for tokens in paraTextDoc if tokens.text.strip() != '']\n",
    "                paraLen = len(paraTextDocList)\n",
    "                paraText = \" \".join(paraTextDocList)\n",
    "                sumParaLen = sumParaLen + paraLen\n",
    "                if paraLen > maxParaLen:\n",
    "                    maxParaLen = paraLen\n",
    "\n",
    "                f.write(paraText + '\\n--------------------------\\n')\n",
    "                \n",
    "\n",
    "            avgParaLen = (sumParaLen/totalParaLen)\n",
    "            self.pageDict[(self.website).split(\"/\")[-1]] = (maxParaLen , avgParaLen)\n",
    "\n",
    "            f.close()\n",
    "            return self.pageDict\n",
    "        \n",
    "\n",
    "\n",
    "class WebsiteParser:\n",
    "    \n",
    "    def __init__(self,baseWebsite,level, pathName):\n",
    "        \n",
    "        self.baseWebsite = baseWebsite\n",
    "        self.level = level\n",
    "        self.pathName = pathName\n",
    "    \n",
    "    def parseWebsite(self):\n",
    "        \n",
    "        pageDict ={}\n",
    "        linkDict = {}\n",
    "        \n",
    "        for level in range(self.level):\n",
    "            \n",
    "            if level == 0:\n",
    "                obj = WebsiteData(self.baseWebsite,level, self.pathName)\n",
    "                objret = obj.parseBaseWebsite()\n",
    "                linkDict[level +1] = obj.retrieveHrefs()\n",
    "                if objret is not None: \n",
    "                    data = obj.saveWebsiteData(level) \n",
    "                    pageDict.update(data)\n",
    "            else:\n",
    "                linkDict[level +1] = []\n",
    "        \n",
    "                if level < (self.level - 1):\n",
    "                \n",
    "                    for link in set(linkDict[level]):\n",
    "                        obj1 = WebsiteData(link, level, self.pathName)\n",
    "                        obj1ret = obj1.parseBaseWebsite()\n",
    "                       \n",
    "                        linkDict[level +1].extend(obj1.retrieveHrefs())\n",
    "                        \n",
    "                        if obj1ret is not None: \n",
    "                            data = obj1.saveWebsiteData(level) \n",
    "                            pageDict.update(data)\n",
    "                \n",
    "                else:\n",
    "                    for link in set(linkDict[level]):\n",
    "                        obj1 = WebsiteData(link, level, self.pathName)\n",
    "                        obj1ret = obj1.parseBaseWebsite()\n",
    "                        \n",
    "                        if obj1ret is not None: \n",
    "                            data = obj1.saveWebsiteData(level) \n",
    "                            pageDict.update(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base url :  https://www.investopedia.com\n",
      "https://www.investopedia.com/terms/f/financialplanner.asp\n",
      "https://www.investopedia.com/terms/r/risk-analysis.asp\n",
      "https://www.investopedia.com/ask/answers/042415/what-difference-between-moral-hazard-and-adverse-selection.asp\n",
      "https://www.investopedia.com/articles/pf/07/risk_tolerance.asp\n",
      "None\n",
      "https://www.investopedia.com/terms/u/ustreasury.asp\n",
      "https://www.investopedia.com/terms/d/derivative.asp\n",
      "https://www.investopedia.com/terms/o/option.asp\n",
      "https://www.investopedia.com/terms/f/futures.asp\n",
      "https://www.investopedia.com/terms/s/subprime-meltdown.asp\n",
      "https://www.investopedia.com/terms/m/mbs.asp\n",
      "None\n",
      "https://www.investopedia.com/terms/b/benchmark.asp\n",
      "https://www.investopedia.com/terms/v/volatility.asp\n",
      "None\n",
      "https://www.investopedia.com/terms/s/standarddeviation.asp\n",
      "https://www.investopedia.com/terms/d/dispersion.asp\n",
      "https://www.investopedia.com/terms/a/annualized-total-return.asp\n",
      "https://www.investopedia.com/terms/s/sp500.asp\n",
      "None\n",
      "https://www.investopedia.com/terms/b/behavioralfinance.asp\n",
      "https://www.investopedia.com/terms/p/prospecttheory.asp\n",
      "https://www.investopedia.com/terms/v/var.asp\n",
      "https://www.investopedia.com/terms/t/timehorizon.asp\n",
      "https://www.investopedia.com/terms/l/longtermcapital.asp\n",
      "https://www.investopedia.com/terms/l/leveragedloan.asp\n",
      "None\n",
      "https://www.investopedia.com/terms/d/drawdown.asp\n",
      "https://www.investopedia.com/terms/b/beta.asp\n",
      "https://www.investopedia.com/terms/c/covariance.asp\n",
      "https://www.investopedia.com/terms/a/activerisk.asp\n",
      "https://www.investopedia.com/terms/t/timeseries.asp\n",
      "https://www.investopedia.com/terms/l/line-of-best-fit.asp\n",
      "None\n",
      "https://www.investopedia.com/terms/f/fundamentalanalysis.asp\n",
      "https://www.investopedia.com/terms/a/alpha-risk.asp\n",
      "None\n",
      "https://www.investopedia.com/terms/i/indexfund.asp\n",
      "https://www.investopedia.com/terms/e/etf.asp\n",
      "https://www.investopedia.com/terms/b/basispoint.asp\n",
      "https://www.investopedia.com/terms/b/beta-risk.asp\n",
      "https://www.investopedia.com/terms/p/portablealpha.asp\n",
      "https://www.investopedia.com/terms/s/sectorrotation.asp\n",
      "https://www.investopedia.com/terms/f/financial-exposure.asp\n",
      "None\n",
      "https://www.investopedia.com/terms/e/equity.asp\n",
      "https://www.investopedia.com/articles/investing-strategy/082816/methods-handling-risk-quick-guide.asp\n",
      "writing data\n",
      "C:\\Users\\piyush.kathuria\\Documents\\Neural_IR\\final_app\\invest-data\\riskmanagement.asp.txt\n",
      "paragraph   :   <p>In the <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/f/financialplanner.asp\">financial</a> world, risk management is the process of identification, <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/r/risk-analysis.asp\">analysis</a> and acceptance or mitigation of uncertainty in investment decisions. Essentially, risk management occurs when an investor or fund manager analyzes and attempts to quantify the potential for losses in an investment, such as a <a data-component=\"link\" data-ordinal=\"3\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/ask/answers/042415/what-difference-between-moral-hazard-and-adverse-selection.asp\">moral hazard</a>, and then takes the appropriate action (or inaction) given his investment objectives and <a data-component=\"link\" data-ordinal=\"4\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/articles/pf/07/risk_tolerance.asp\">risk tolerance</a>.</p>\n",
      "paragraph   :   <p>Risk management occurs everywhere in the realm of finance. It occurs when an investor buys <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/u/ustreasury.asp\">U.S. Treasury</a> bonds over corporate bonds, when a fund manager hedges his currency exposure with currency <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/d/derivative.asp\">derivatives</a>, and when a bank performs a credit check on an individual before issuing a personal line of credit. Stockbrokers use financial instruments like <a data-component=\"link\" data-ordinal=\"3\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/o/option.asp\">options</a> and <a data-component=\"link\" data-ordinal=\"4\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/f/futures.asp\">futures</a>, and money managers use strategies like portfolio and investment diversification to mitigate or effectively manage risk.</p>\n",
      "paragraph   :   <p>Inadequate risk management can result in severe consequences for companies, individuals, and the economy. For example, the <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/s/subprime-meltdown.asp\">subprime mortgage meltdown</a> in 2007 that helped trigger the Great Recession stemmed from bad risk-management decisions, such as lenders who extended mortgages to individuals with poor credit; investment firms who bought, packaged, and resold these mortgages; and funds that invested excessively in the repackaged, but still risky, <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/m/mbs.asp\">mortgage-backed securities (MBS)</a>.</p>\n",
      "paragraph   :   <p>We tend to think of \"risk\" in predominantly negative terms. However, in the investment world, the risk is necessary and inseparable from the performance.</p>\n",
      "paragraph   :   <p>A common definition of investment risk is a <em>deviation from an expected outcome</em>. We can express this deviation in absolute terms or relative to something else, like a market <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/b/benchmark.asp\">benchmark</a>. That deviation can be positive or negative, and it relates to the idea of \"no pain, no gain\": to achieve higher returns, in the long run, you have to accept the more short-term risk, in the shape of <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/v/volatility.asp\">volatility</a>.</p>\n",
      "paragraph   :   <p>How much volatility depends on your risk tolerance, which is an expression of the <em>capacity</em> to assume volatility based on specific financial circumstances and the <em>propensity</em> to do so, taking into account your psychological comfort with uncertainty and the possibility of incurring large short-term losses.</p>\n",
      "paragraph   :   <p>Investors use a variety of tactics to ascertain risk. One of the most commonly used absolute risk metrics is <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/s/standarddeviation.asp\">standard deviation</a>, a statistical measure of <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/d/dispersion.asp\">dispersion</a> around a central tendency. You look at the average return of an investment and then find its average standard deviation over the same time period. Normal distributions (the familiar bell-shaped curve) dictate that the expected return of the investment is likely to be one standard deviation from the average 67% of the time and two standard deviations from the average deviation 95% of the time. This helps investors evaluate risk numerically. If they believe that they can tolerate the risk, financially and emotionally, they invest.</p>\n",
      "paragraph   :   <p>For example, during a 15-year period from August 1, 1992, to July 31, 2007, the average <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/a/annualized-total-return.asp\">annualized total return</a> of the <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/s/sp500.asp\">S&amp;P 500 </a> was 10.7%. This number reveals what happened for the whole period, but it does not say what happened along the way. The average standard deviation of the S&amp;P 500 for that same period was 13.5%. This is the difference between the average return and the real return at most given points throughout the 15-year period.</p>\n",
      "paragraph   :   <p>When applying the bell curve model, any given outcome should fall within one standard deviation of the mean about 67% of the time and within two standard deviations about 95% of the time. Thus, an S&amp;P 500 investor could expect the return, at any given point during this period, to be 10.7% plus or minus the standard deviation of 13.5% about 67% of the time; he may also assume a 27% (two standard deviations) increase or decrease 95% of the time. If he can afford the loss, he invests.</p>\n",
      "paragraph   :   <p>While that information may be helpful, it does not fully address an investor's risk concerns. The field of <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/b/behavioralfinance.asp\">behavioral finance</a> has contributed an important element to the risk equation, demonstrating asymmetry between how people view gains and losses. In the language of <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/p/prospecttheory.asp\">prospect theory</a>, an area of behavioral finance introduced by Amos Tversky and Daniel Kahneman in 1979, investors exhibit <em>loss aversion</em>: They put more weight on the pain associated with a loss than the good feeling associated with a profit.</p>\n",
      "paragraph   :   <p>Often, what investors really want to know is not just how much an asset deviates from its expected outcome, but how bad things look way down on the left-hand tail of the distribution curve. <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/v/var.asp\">Value at risk</a> (VAR) attempts to provide an answer to this question. The idea behind VAR is to quantify how large a loss on investment could be with a given level of confidence over a defined period. For example, the following statement would be an example of VAR: \"With about a 95% level of confidence, the most you stand to lose on this $1,000 investment over a two-year <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/t/timehorizon.asp\">time horizon</a> is $200.\" The confidence level is a probability statement based on the statistical characteristics of the investment and the shape of its distribution curve. </p>\n",
      "paragraph   :   <p>Of course, even a measure like VAR doesn't guarantee that 5% of the time will be much worse. Spectacular debacles like the one that hit the hedge fund <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/l/longtermcapital.asp\">Long-Term Capital Management</a> in 1998 remind us that so-called \"outlier events\" may occur. In the case of LTCM, the outlier event was the Russian government's default on its outstanding sovereign debt obligations, an event that threatened to bankrupt the hedge fund, which had highly<a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/l/leveragedloan.asp\"> leveraged</a> positions worth over $1 trillion; if it had gone under, it could have collapsed the global financial system. The U.S. government created a $3.65-billion loan fund to cover LTCM's losses, which enabled the firm to survive the market volatility and liquidate in an orderly manner in early 2000.</p>\n",
      "paragraph   :   <p>Another risk measure oriented to behavioral tendencies is a <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/d/drawdown.asp\">drawdown</a>, which refers to any period during which an asset's return is negative relative to a previous high mark. In measuring drawdown, we attempt to address three things:</p>\n",
      "paragraph   :   <p>For example, in addition to wanting to know whether a mutual fund beat the S&amp;P 500, we also want to know how comparatively risky it was. One measure for this is <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/b/beta.asp\">beta</a> (known as \"market risk\"), based on the statistical property of <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/c/covariance.asp\">covariance</a>. A beta greater than 1 indicates more risk than the market and vice versa.</p>\n",
      "paragraph   :   <p>Beta helps us to understand the concepts of passive and <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/a/activerisk.asp\">active risk</a>. The graph below shows a <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/t/timeseries.asp\">time series</a> of returns (each data point labeled \"+\") for a particular portfolio R(p) versus the market return R(m). The returns are cash-adjusted, so the point at which the x and y-axes intersect is the cash-equivalent return. Drawing a <a data-component=\"link\" data-ordinal=\"3\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/l/line-of-best-fit.asp\">line of best fit</a> through the data points allows us to quantify the passive risk (beta) and the active risk (alpha).</p>\n",
      "paragraph   :   <p>The gradient of the line is its beta. For example, a gradient of 1.0 indicates that for every unit increase of market return, the portfolio return also increases by one unit. A money manager employing a passive management strategy can attempt to increase the portfolio return by taking on more market risk (i.e., a beta greater than 1) or alternatively decrease portfolio risk (and return) by reducing the portfolio beta below 1.</p>\n",
      "paragraph   :   <p>If the level of market or systematic risk were the only influencing factor, then a portfolio's return would always be equal to the beta-adjusted market return. Of course, this is not the case: Returns vary because of a number of factors unrelated to market risk. Investment managers who follow an active strategy take on other risks to achieve excess returns over the market's performance. Active strategies include stock, sector or country selection, <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/f/fundamentalanalysis.asp\">fundamental analysis</a>, and charting.</p>\n",
      "paragraph   :   <p>Active managers are on the hunt for an alpha, the measure of excess return. In our diagram example above, alpha is the amount of portfolio return not explained by beta, represented as the distance between the intersection of the x and y-axes and the y-axis intercept, which can be positive or negative. In their quest for excess returns, active managers expose investors to <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/a/alpha-risk.asp\">alpha risk</a>, the risk that the result of their bets will prove negative rather than positive. For example, a fund manager may think that the energy sector will outperform the S&amp;P 500 and increase her portfolio's weighting in this sector. If unexpected economic developments cause energy stocks to sharply decline, the manager will likely underperform the benchmark, an example of alpha risk.</p>\n",
      "paragraph   :   <p>In general, the more active the investment strategy (the more alpha a fund manager seeks to generate), the more an investor will need to pay for exposure to that strategy. For a purely passive vehicle like an <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/i/indexfund.asp\">index fund</a> or an <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/e/etf.asp\">exchange-traded fund (ETF</a>), you might pay 15 to 20 <a data-component=\"link\" data-ordinal=\"3\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/b/basispoint.asp\">basis points</a> in annual management fees, while for a high-octane hedge fund employing complex trading strategies involving high capital commitments and transaction costs, an investor would need to pay 200 basis points in annual fees, plus give back 20% of the profits to the manager.</p>\n",
      "paragraph   :   <p>The difference in pricing between passive and active strategies (or <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/b/beta-risk.asp\">beta risk</a> and alpha risk respectively) encourages many investors to try and separate these risks (e.g. to pay lower fees for the beta risk assumed and concentrate their more expensive exposures to specifically defined alpha opportunities). This is popularly known as <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/p/portablealpha.asp\">portable alpha</a>, the idea that the alpha component of a total return is separate from the beta component.</p>\n",
      "paragraph   :   <p>For example, a fund manager may claim to have an active <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/s/sectorrotation.asp\">sector rotation</a> strategy for beating the S&amp;P 500 and show, as evidence, a track record of beating the index by 1.5% on an average annualized basis. To the investor, that 1.5% of excess return is the manager's value, the alpha, and the investor is willing to pay higher fees to obtain it. The rest of the total return, what the S&amp;P 500 itself earned, arguably has nothing to do with the manager's unique ability. Portable alpha strategies use derivatives and other tools to refine how they obtain and pay for the alpha and beta components of their <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/f/financial-exposure.asp\">exposure</a>.</p>\n",
      "paragraph   :   <p>Risk is inseparable from return. Every investment involves some degree of risk, which can be very close to zero in the case of a U.S. T-bill or very high for something such as concentrated exposure to Sri Lankan <a data-component=\"link\" data-ordinal=\"1\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/terms/e/equity.asp\">equities</a> or real estate in Argentina. Risk is quantifiable both in absolute and in relative terms. A solid understanding of risk in its different forms can help investors to better understand the opportunities, trade-offs, and costs involved with different <a data-component=\"link\" data-ordinal=\"2\" data-source=\"inlineLink\" data-type=\"internalLink\" href=\"https://www.investopedia.com/articles/investing-strategy/082816/methods-handling-risk-quick-guide.asp\">investment approaches</a>.</p>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "numLevels = 1\n",
    "initLink = 'https://www.investopedia.com/terms/r/riskmanagement.asp'\n",
    "path = r\"C:\\Users\\piyush.kathuria\\Documents\\Neural_IR\\final_app\\invest-data\"\n",
    "obj = WebsiteParser(initLink,numLevels,path)\n",
    "print(obj.parseWebsite())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentRanker():\n",
    "    \n",
    "    def __init__(self, query, folderPath):\n",
    "        \n",
    "        self.documentFolderPath = folderPath\n",
    "        print(\"folderPath  : \", folderPath)\n",
    "        self.query = self.convertQuery(query)\n",
    "        self.folderDocumentContent = self.loadDocuments()  # Store the content of all the files in dictionary\n",
    "        self.bm25Model = self.createBM25Model()\n",
    "    \n",
    "    \n",
    "    def convertQuery(self,query):\n",
    "        \n",
    "        nlp = spacy.load(\"en_core_web_sm\",disable = ['ner', 'parser', 'textcat'])\n",
    "        doc = nlp(query.lower())\n",
    "        return doc\n",
    "        \n",
    "    def loadDocuments(self):\n",
    "        '''\n",
    "        Load documents from the invest_data folder into a dictionary\n",
    "        '''\n",
    "        documentTuple = []\n",
    "        nounList = self.getNouns()\n",
    "        nlp = spacy.load(\"en_core_web_sm\",disable = ['ner', 'parser', 'textcat'])\n",
    "        \n",
    "        for root, dirs, files in os.walk(self.documentFolderPath):\n",
    "    \n",
    "            for file in files:\n",
    "                if file.endswith('txt'):\n",
    "                    with open(os.path.abspath(os.path.join(root, file)), 'r' , encoding=\"utf8\" ) as f:\n",
    "                        content = f.read()\n",
    "                        content = content.lower()\n",
    "                        doc = nlp(content)\n",
    "                        tokenizedContent = [tokens.text for tokens in doc if tokens.text]         # Change 1 \n",
    "                        documentTuple.append((file,tokenizedContent))\n",
    "        \n",
    "        \n",
    "        return documentTuple\n",
    "    \n",
    "        \n",
    "    \n",
    "    def getNouns(self):\n",
    "        \n",
    "        #nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        '''\n",
    "        spacy_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "        \n",
    "        querySplit = self.query.lower().split()\n",
    "        \n",
    "        querySplit = [x for x in querySplit if x not in spacy_stopwords]\n",
    "        \n",
    "        print(querySplit)\n",
    "        \n",
    "        return querySplit\n",
    "        \n",
    "        '''\n",
    "        nounList = []\n",
    "        for token in self.query:\n",
    "            #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" or token.pos_ == \"ADJ\":\n",
    "                print(token.text)\n",
    "                print(token.pos_)\n",
    "                nounList.append(token.text)\n",
    "            \n",
    "        print(\"noun list : \", nounList)        \n",
    "        return nounList\n",
    "    \n",
    "        \n",
    "    def createBM25Model(self):\n",
    "        '''\n",
    "        Return a created BM25 model.\n",
    "        '''\n",
    "        return gensim.summarization.bm25.BM25([x[1] for x in self.folderDocumentContent])\n",
    "    \n",
    "    def rankDocuments(self):\n",
    "        '''\n",
    "        Rank the documents in the corpus wrt the query. \n",
    "        '''\n",
    "        query = [tokens.text for tokens in self.query if not tokens.is_punct]\n",
    "        scores = self.bm25Model.get_scores(query)\n",
    "        documentScores =  [(self.folderDocumentContent[i][0],scores[i]) for i in range(len(scores))]\n",
    "        return documentScores\n",
    "    \n",
    "    def returnTopK(self,k=5):\n",
    "        '''\n",
    "        Return top k ranked documents wrt given user query\n",
    "        '''\n",
    "        \n",
    "        scoreDict = self.rankDocuments()\n",
    "        return sorted(scoreDict , key = lambda x: x[1], reverse = True)[:k]\n",
    "    \n",
    "    def returnTopDocuments(self):\n",
    "        '''\n",
    "        Return top documents wrt given user query. This uses a different approach from our \n",
    "        previous function. We return all documents with score a standard deviation above the\n",
    "        mean score of our corpus\n",
    "        '''\n",
    "        \n",
    "        scoreTup = self.rankDocuments()\n",
    "        #scoreDict = dict(scoreTup)\n",
    "        scores = np.array([x[1] for x in scoreTup])\n",
    "        #scores = np.array(list(scoreDict.values()))\n",
    "        meanScore = np.mean(scores)\n",
    "        sdScore = np.std(scores)\n",
    "        maxThreshold = meanScore + 2*sdScore\n",
    "        topScoresDict = [x for x in scoreTup if x[1] > maxThreshold]\n",
    "        topDocumentScores = sorted(topScoresDict, key = lambda x: x[1], reverse = True)\n",
    "        \n",
    "        return topDocumentScores\n",
    "    \n",
    "    def returnTopDocumentsData(self):\n",
    "        '''\n",
    "        Return data from the top documents retrieved by returnTopDocuments\n",
    "        '''\n",
    "        #query = self.query\n",
    "        topDocumentScores = self.returnTopDocuments()\n",
    "        print(\"topDocumentScores  : \", topDocumentScores)\n",
    "        topDocumentNames = [x[0] for x in topDocumentScores]\n",
    "        #print(topDocumentNames)\n",
    "        \n",
    "        topDocumentText = [x for x in self.folderDocumentContent if x[0] in topDocumentNames]\n",
    "        return topDocumentText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PassageRanker():\n",
    "    \n",
    "    def __init__(self, query, topDocumentContent):\n",
    "        \n",
    "        self.query = self.convertQuery(query)\n",
    "        self.topDocumentContent = topDocumentContent\n",
    "        #self.BM25Model = self.createBM25Model()\n",
    "    \n",
    "    def convertQuery(self,query):\n",
    "        \n",
    "        nlp = spacy.load(\"en_core_web_sm\",disable = ['ner', 'parser', 'textcat'])\n",
    "        doc = nlp(query.lower())\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def createBM25Model(self,documentContent):\n",
    "        '''\n",
    "        Return a created BM25 model.\n",
    "        '''\n",
    "        #return gensim.summarization.bm25.BM25([x[1] for x in self.topDocumentContent])\n",
    "        return gensim.summarization.bm25.BM25(documentContent)\n",
    "    \n",
    "    \n",
    "    def getNouns(self):\n",
    "        \n",
    "        #nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        '''\n",
    "        spacy_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "        \n",
    "        querySplit = self.query.lower().split()\n",
    "        \n",
    "        querySplit = [x for x in querySplit if x not in spacy_stopwords]\n",
    "        \n",
    "        print(querySplit)\n",
    "        \n",
    "        return querySplit\n",
    "        \n",
    "        '''\n",
    "        nounList = []\n",
    "        for token in self.query:\n",
    "            #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" or token.pos_ == \"ADJ\":\n",
    "                nounList.append(token.text)\n",
    "            \n",
    "        #print(\"noun list : \", nounList)        \n",
    "        return nounList\n",
    "    \n",
    "    def checkNounMatch(self,paragraph,nounList):\n",
    "        '''\n",
    "        A function to see if each noun is matching for a paragraph\n",
    "        '''\n",
    "        nounBool = True\n",
    "        \n",
    "        #nlp = spacy.load(\"en_core_web_sm\",disable=['ner', 'parser', 'textcat'])\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        paragraphContent = \" \".join([stemmer.stem(token) for token in paragraph])\n",
    "        \n",
    "        \n",
    "        for nouns in nounList:\n",
    "            \n",
    "            #paragraphDoc = nlp(paragraphContent)\n",
    "            #paragraphLemma = [tokens.lemma_ for tokens in paragraphDoc]\n",
    "            \n",
    "            if stemmer.stem(nouns) in paragraphContent:\n",
    "                nounBool = True\n",
    "            else:\n",
    "                nounBool = False\n",
    "                break\n",
    "        \n",
    "        return nounBool\n",
    "                \n",
    "            \n",
    "    def returnParagraphList(self,content):\n",
    "        '''\n",
    "        Return pargaraph in the form of list of lists to be fed to the BM25 Model\n",
    "        '''\n",
    "        paragraphList = []\n",
    "        paragraph = content#[sepCounter:i] # \" \".join() <-\n",
    "        nounList = self.getNouns()\n",
    "        if self.checkNounMatch(paragraph,nounList):\n",
    "                #if all(nouns in paragraph for nouns in nounList):\n",
    "                   \n",
    "            paragraphList.append(paragraph)\n",
    "        return paragraphList\n",
    "        '''\n",
    "        paragraphList = []\n",
    "        sepCounter = 0\n",
    "        nounList = self.getNouns()\n",
    "\n",
    "        print(nounList)\n",
    "        for i,term in enumerate(content):\n",
    "            \n",
    "            if term == \"--------------------------\":\n",
    "                paragraph = content[sepCounter:i] # \" \".join() <-\n",
    "                print(\"para : \")\n",
    "                print(paragraph)\n",
    "                if self.checkNounMatch(paragraph,nounList):\n",
    "                #if all(nouns in paragraph for nouns in nounList):\n",
    "                   \n",
    "                    paragraphList.append(paragraph)\n",
    "                \n",
    "                sepCounter = i+1\n",
    "        return paragraphList\n",
    "        '''\n",
    "        \n",
    "    \n",
    "    \n",
    "    def returnParagraphScores(self,bm25Model,query):\n",
    "        '''\n",
    "        Return the score of a paragraph given a model.\n",
    "        '''\n",
    "        return bm25Model.get_scores(query)\n",
    "        \n",
    "    \n",
    "    def returnTopPassages(self, k=10):\n",
    "        '''\n",
    "        Rank each paragraph from the document and return the top 10 passages from the collection of documents\n",
    "        '''\n",
    "        query = [tokens.text for tokens in self.query if not tokens.is_punct]\n",
    "        \n",
    "        #documentParagraphScores = [(document[0],self.rankPassages(document[1])) for document in topDocumentContent]\n",
    "        paragraphScoreTup = []\n",
    "        paragraphLoL = []\n",
    "        for documents in self.topDocumentContent:\n",
    "            #print(documents)\n",
    "            paragraphLoL.extend(self.returnParagraphList(documents[1]))\n",
    "        \n",
    "        \n",
    "        print(len(paragraphLoL))\n",
    "        #passageBM25Model = self.createBM25Model(paragraphLoL)\n",
    "        \n",
    "        #for documents in self.topDocumentContent:\n",
    "        #paragraphScores = self.returnParagraphScores(passageBM25Model, query)\n",
    "        for i,paragraph in enumerate(paragraphLoL):\n",
    "                \n",
    "            #paragraphScoreTup.append((paragraph,paragraphScores[i]))\n",
    "            paragraphScoreTup.append((paragraph,1))\n",
    "        \n",
    "        print(\" length of paragraphScoreTup : \", len(paragraphScoreTup))\n",
    "        \n",
    "        self.topParagraphScores = sorted(paragraphScoreTup, key = lambda x: x[1], reverse = True)\n",
    "        \n",
    "        return self.topParagraphScores#[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bertQAModel():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.tokenizerModel,self.bertQAModel = self.__initializeModel()\n",
    "    \n",
    "    def __initializeModel(self):\n",
    "        '''\n",
    "        Initialize the Beret Tokenizer and the QA model. Note that this is currently compatible with\n",
    "        transformers module, NOT pytorch/tensorflow\n",
    "        '''\n",
    "        with open(\"./models/bertTokenizer.pkl\",\"rb\") as f:\n",
    "            tokenizerModel = pickle.load(f)\n",
    "        \n",
    "        with open(\"./models/bertQAModel.pkl\",\"rb\") as f:\n",
    "            bertQAModel = pickle.load(f)\n",
    "        \n",
    "        return tokenizerModel, bertQAModel\n",
    "    \n",
    "    def stringProcess(self,answer):\n",
    "        answerSplit = answer.split(\" ##\")\n",
    "        return \"\".join(answerSplit)\n",
    "    \n",
    "    def predict(self,text,question):\n",
    "        '''\n",
    "        Predict the answer given a passage and a question.\n",
    "        '''\n",
    "        input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\n",
    "        print(\"INPUT_TEXT : \")\n",
    "        print(input_text)\n",
    "        input_ids = self.tokenizerModel.encode(input_text)\n",
    "        #print(\"TOKENIZED TEXT : \")\n",
    "        #print(input_ids)\n",
    "        token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n",
    "        #print(\"TOKEN TYPE IDS : \")\n",
    "        #print(token_type_ids)\n",
    "        start_scores, end_scores = self.bertQAModel(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
    "        \n",
    "        \n",
    "        # Normalized scores\n",
    "        #startScoresVec = ( (startScoresVec - np.mean(startScoresVec))/np.std(startScoresVec))\n",
    "        #endScoresVec = ( (endScoresVec - np.mean(endScoresVec))/np.std(endScoresVec))\n",
    "        \n",
    "        # Normalized Scores\n",
    "        #startScoresVec = scale(np.reshape(startScoresVec,newshape=(-1,1)))\n",
    "        #endScoresVec = scale(np.reshape(endScoresVec,newshape=(-1,1)))\n",
    "        \n",
    "        # Normalized Scores\n",
    "        #softmax = torch.nn.Softmax()\n",
    "        #start_scores = softmax(start_scores)\n",
    "        #end_scores = softmax(end_scores)\n",
    "        #endScoresVec = scale(np.reshape(endScoresVec,newshape=(-1,1)))\n",
    "        \n",
    "        #startScoresVec = ( (startScoresVec - np.min(startScoresVec))/np.ptp(startScoresVec))\n",
    "        #endScoresVec = ( (endScoresVec - np.min(endScoresVec))/np.ptp(endScoresVec))\n",
    "        \n",
    "        \n",
    "        all_tokens = self.tokenizerModel.convert_ids_to_tokens(input_ids)\n",
    "        answer = ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n",
    "        startScoreInd,endScoreInd = torch.argmax(start_scores),torch.argmax(end_scores)\n",
    "        startScoresVec = start_scores.detach().numpy().flatten()\n",
    "        endScoresVec = end_scores.detach().numpy().flatten()\n",
    "        \n",
    "        startScoreMax, endScoreMax = startScoresVec[startScoreInd] , endScoresVec[endScoreInd]\n",
    "        #avgScore = float(np.absolute(startScoreMax) + np.absolute(endScoreMax))\n",
    "        #avgScore = float(startScoreMax + endScoreMax)\n",
    "        print(\"ANSWER RETRIEVED : \")\n",
    "        print(self.stringProcess(answer))\n",
    "        return (self.stringProcess(answer),startScoreMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\Users\\piyush.kathuria\\Documents\\Neural_IR\\final_app\\invest-data\n",
    "# C:\\Users\\piyush.kathuria\\Desktop\\invest-data\n",
    "path = r\"C:\\Users\\piyush.kathuria\\Documents\\Neural_IR\\final_app\\invest-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are two common types of mbss?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folderPath  :  C:\\Users\\piyush.kathuria\\Documents\\Neural_IR\\final_app\\invest-data\n",
      "common\n",
      "ADJ\n",
      "types\n",
      "NOUN\n",
      "mbss\n",
      "NOUN\n",
      "noun list :  ['common', 'types', 'mbss']\n",
      "topDocumentScores  :  [('mbs8.txt', 27.18925069326761), ('correlationcoefficient2.txt', 14.69516599541684), ('financialasset8.txt', 14.279465651351543), ('normaldistribution1.txt', 13.869377882774158), ('regression6.txt', 13.139476535879528), ('scenario_analysis5.txt', 11.878436523749176), ('what-difference-between-derivatives-and-options9.txt', 11.842945492518437), ('mbs20.txt', 11.258308264533198), ('optionscontract1.txt', 11.078312346770689), ('regression2.txt', 11.019815578199598), ('mbs19.txt', 10.866760332537586), ('certificateofdeposit15.txt', 10.528099526921242), ('security1.txt', 10.339108392102153), ('activerisk16.txt', 10.160073981995781), ('mutualfund9.txt', 10.073669632391162), ('equity9.txt', 10.052890117388632), ('equity9.txt', 10.052890117388632), ('patent4.txt', 10.052890117388632), ('etf5.txt', 9.943945492305081), ('gdp3.txt', 9.939141717247482), ('mbs18.txt', 9.895279557462331), ('foreignexchangerisk4.txt', 9.82793864189827), ('behavioralfinance9.txt', 9.810407604766153), ('contraaccount5.txt', 9.758747294782259), ('exchange-traded-derivative1.txt', 9.604595123513047), ('gametheory12.txt', 9.583977268893486), ('mezzaninefinancing4.txt', 9.551813591389463), ('financialplanner9.txt', 9.546867943293964), ('what-difference-between-paidup-capital-and-share-capital9.txt', 9.53480379630869), ('equity2.txt', 9.531183870819543), ('equity2.txt', 9.531183870819543), ('swaprate5.txt', 9.406944096136543), ('put4.txt', 9.38865285786612), ('financialinstrument4.txt', 9.36930848865439), ('mutual-fund-etf7.txt', 9.261242085540356), ('modernportfoliotheory6.txt', 9.229063099055946), ('equity25.txt', 9.179328106550374), ('equity25.txt', 9.179328106550374), ('preservationofcapital2.txt', 9.118489552566784), ('3-s-simple-investing0.txt', 9.086160220672012), ('mbs14.txt', 9.062908464466055), ('basics-options-profitability24.txt', 9.020535863762934), ('swaprate1.txt', 9.006358905892014), ('financial-advisor6.txt', 8.912378738070164), ('averagereturn3.txt', 8.891996407967072), ('federal-reserve-note8.txt', 8.841755022189686), ('moneysupply5.txt', 8.830141014293922), ('fixed-incomesecurity6.txt', 8.817094913364587), ('taxshelter1.txt', 8.80502530559566), ('how-standard-deviation-used-determine-risk12.txt', 8.798357144479402), ('investmentvehicle1.txt', 8.774891915816061), ('risk14.txt', 8.750939667539331), ('futurescontract4.txt', 8.690274993883712), ('vanillaoption3.txt', 8.657502873986179), ('sharperatio11.txt', 8.648092947705905), ('risk12.txt', 8.585886318158082), ('speculateoptions0.txt', 8.57614895837872), ('interestrate11.txt', 8.516637979584594), ('expirationdate6.txt', 8.50342913578814), ('equityfinancing3.txt', 8.499437515644038), ('what-difference-between-moral-hazard-and-adverse-selection9.txt', 8.419282796911194), ('price_level3.txt', 8.348983706447877), ('maximum-foreseeable-loss9.txt', 8.345243873788927), ('vanillaoption13.txt', 8.341171969911533), ('how-do-sp-500-futures-work0.txt', 8.30897641458577), ('certificateofdeposit47.txt', 8.296803704013119), ('balancesheet19.txt', 8.24277973720855), ('technicalanalysis14.txt', 8.230025661373478), ('corporatecharter7.txt', 8.189246591940163), ('financialasset3.txt', 8.16752768001637), ('mergersandacquisitions13.txt', 8.163540363767808), ('mutualfund10.txt', 8.115095082799181), ('angelinvestor1.txt', 8.10766363936414), ('riskadjustedreturn2.txt', 8.095242051195076), ('etn0.txt', 8.088546654107962), ('real-property3.txt', 8.086172814988334), ('interestrateswap3.txt', 8.030884064786274), ('commercialbank1.txt', 8.01439209607809), ('fiduciary6.txt', 8.007664675921374), ('inflation10.txt', 7.996180477267983), ('treasurybond1.txt', 7.969240650663984), ('government-bond21.txt', 7.958126247912663), ('syndicate1.txt', 7.958126247912663), ('mutual-fund-etf1.txt', 7.922767925429289), ('valuation7.txt', 7.879154595170123), ('fixed-incomesecurity18.txt', 7.838845055751752), ('stockmarket26.txt', 7.838845055751752), ('arbitrage2.txt', 7.815950066150044), ('putoption6.txt', 7.78277651952565), ('risk7.txt', 7.70651833504532), ('inflation4.txt', 7.700559017342486), ('bullcallspread17.txt', 7.699955925483615), ('nber6.txt', 7.669216480121103), ('security16.txt', 7.649038895534283), ('outstandingshares18.txt', 7.646996828666229), ('how-covariance-used-portfolio-theory0.txt', 7.625805012658689), ('3-s-simple-investing5.txt', 7.6231084611623725), ('liability2.txt', 7.6129520791814045), ('financial-crisis-review10.txt', 7.597845424703996), ('nikkei2.txt', 7.524911731974061), ('financialinstrument2.txt', 7.507026266152394), ('pipe0.txt', 7.504745681044924), ('covariance4.txt', 7.489958391436474), ('financialinstrument0.txt', 7.483896819163621), ('nber1.txt', 7.483896819163621), ('fixed-incomesecurity1.txt', 7.459637337496439), ('certificateofdeposit71.txt', 7.444670072857853), ('etf1.txt', 7.438742702303296), ('gametheory8.txt', 7.431557665604295), ('balancesheet6.txt', 7.411866593591405), ('longtermcapital4.txt', 7.409425815962651), ('market_cycles0.txt', 7.406223513082249), ('forwardcontract8.txt', 7.394510610042023), ('inflation3.txt', 7.37802323069247), ('benchmark1.txt', 7.350042577412141), ('benchmark1.txt', 7.350042577412141), ('risk-analysis15.txt', 7.341546083430996), ('derivative19.txt', 7.32984945831524), ('derivative19.txt', 7.32984945831524), ('investmentvehicle2.txt', 7.312801692945323), ('tax-planning1.txt', 7.312311251787577), ('equityfinancing4.txt', 7.285649654822828), ('annualized-total-return1.txt', 7.279767544716714), ('pipe5.txt', 7.277058967869276), ('certificateofdeposit56.txt', 7.271549262017813), ('behavioraleconomics0.txt', 7.259569481486572), ('investmentvehicle14.txt', 7.250958293339826), ('fiduciary2.txt', 7.243004299510451), ('marketablesecurities3.txt', 7.24048751808757), ('derivative10.txt', 7.236915047434849), ('derivative10.txt', 7.236915047434849), ('nav16.txt', 7.227051729131949), ('behavioralfinance2.txt', 7.207875322961247), ('subprime_mortgage4.txt', 7.195762755988602), ('stakeholder2.txt', 7.179609168767174), ('what-difference-between-standard-deviation-and-average-deviation1.txt', 7.174907601374214), ('risk34.txt', 7.15233794968862), ('financial-advisor2.txt', 7.150673201747203), ('ironcondor6.txt', 7.144313559352181), ('risk-analysis1.txt', 7.143292266057443), ('holdings1.txt', 7.14209252873626), ('risk26.txt', 7.138804692208771), ('seasonal-adjustment3.txt', 7.112621461690088), ('fundamentalanalysis36.txt', 7.087545032410361), ('algorithm0.txt', 7.075648342584779), ('random-variable11.txt', 7.073966181326888), ('what-difference-between-standard-deviation-and-average-deviation0.txt', 7.056433074742863), ('correlationcoefficient11.txt', 7.051579542824079), ('diversification3.txt', 7.046931271057042), ('benchmarkerror3.txt', 7.041945866509449), ('economicgrowth9.txt', 7.0199971360333056), ('underlying-asset7.txt', 7.0199971360333056), ('random-variable10.txt', 7.01920797204836), ('tobacco-tax1.txt', 7.006032506973153), ('optionscontract4.txt', 7.004738001449505), ('price_level4.txt', 7.004738001449505), ('scenario_analysis2.txt', 7.001800054395675), ('certificateofdeposit58.txt', 6.989511839495881), ('trackingerror18.txt', 6.977571106433199), ('02140315.txt', 6.974357949428395), ('treasurybill4.txt', 6.952994499789018), ('what-does-standard-deviation-measure-portfolio4.txt', 6.950780698263754), ('stockmarket7.txt', 6.932548758812686), ('what-are-components-shareholders-equity1.txt', 6.9244594610009935), ('incomestatement29.txt', 6.920509613228153), ('futuresmarket2.txt', 6.9180769857722435), ('proprietarytechnology6.txt', 6.915392529301606), ('technicalanalysis13.txt', 6.914453113311382), ('security21.txt', 6.897715434398343), ('fiduciary26.txt', 6.897668902619924), ('inthemoney5.txt', 6.892090761148486), ('dividend0.txt', 6.887405326500595), ('syndicate6.txt', 6.871390818233378), ('sample11.txt', 6.859994778194405), ('behavioralfinance0.txt', 6.8551821012653615), ('bullcallspread7.txt', 6.849445048907956), ('beta13.txt', 6.832202845647474), ('beta13.txt', 6.832202845647474), ('risk1.txt', 6.823893579645022), ('form45.txt', 6.806473286123612), ('riskadjustedreturn1.txt', 6.797431198721036), ('forwardcontract4.txt', 6.79430532262461), ('liquidmarket0.txt', 6.791584455689315), ('equity1.txt', 6.78382968638126), ('equity1.txt', 6.78382968638126), ('commodity-etf6.txt', 6.7836089129052475), ('riskmanagement10.txt', 6.775024162657765), ('riskmanagement10.txt', 6.775024162657765), ('marketablesecurities17.txt', 6.754280699928268), ('gdp27.txt', 6.746973976220415), ('risk5.txt', 6.743386592128884), ('notionalvalue12.txt', 6.741427732898125), ('sp-500-vs-russell-2000-etf-which-should-you-get17.txt', 6.727674133517812), ('futures2.txt', 6.72721122176706), ('maximum-foreseeable-loss5.txt', 6.708813705792719), ('what-difference-between-paidup-capital-and-share-capital1.txt', 6.708813705792719), ('price_level7.txt', 6.705187318453362), ('gse3.txt', 6.703177186563178), ('mergersandacquisitions0.txt', 6.703177186563178), ('calculating-covariance0.txt', 6.701629465708282), ('least-squares-method1.txt', 6.700949438280826), ('correlationcoefficient0.txt', 6.6860670510831195), ('stockmarket36.txt', 6.655699175121443), ('vanillaoption14.txt', 6.655276203439076), ('type-ii-error7.txt', 6.65083090783543), ('basics-options-profitability0.txt', 6.646872216391476), ('basispoint2.txt', 6.64456687911077), ('sensitivityanalysis2.txt', 6.636190721642144), ('sector0.txt', 6.628496351390005), ('risk2.txt', 6.622266059305012), ('tranches8.txt', 6.617371603176704), ('option-premium0.txt', 6.616069356659032), ('riskmanagement7.txt', 6.613382455592566), ('riskmanagement7.txt', 6.613382455592566), ('correlationcoefficient9.txt', 6.596703196008397), ('fiduciary22.txt', 6.594108456634638), ('how-standard-deviation-used-determine-risk10.txt', 6.58542871931867)]\n"
     ]
    }
   ],
   "source": [
    "docRanker = DocumentRanker(query,path)\n",
    "topDocuments = docRanker.returnTopDocumentsData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      " length of paragraphScoreTup :  1\n"
     ]
    }
   ],
   "source": [
    "passageRanker = PassageRanker(query , topDocuments)\n",
    "passageRanking = passageRanker.returnTopPassages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['there',\n",
       "   'are',\n",
       "   'two',\n",
       "   'common',\n",
       "   'types',\n",
       "   'of',\n",
       "   'mbss',\n",
       "   ':',\n",
       "   'pass',\n",
       "   '-',\n",
       "   'throughs',\n",
       "   'and',\n",
       "   'collateralized',\n",
       "   'mortgage',\n",
       "   'obligations',\n",
       "   '(',\n",
       "   'cmo',\n",
       "   ')',\n",
       "   '.'],\n",
       "  1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passageRanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_TEXT : \n",
      "[CLS] What are two common types of mbss [SEP] there are two common types of mbss : pass - throughs and collateralized mortgage obligations ( cmo ) . [SEP]\n",
      "ANSWER RETRIEVED : \n",
      "pass - throughs and collateralized mortgage obligations ( cmo )\n"
     ]
    }
   ],
   "source": [
    "bertQA = bertQAModel()\n",
    "answerTuple = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "queryDoc = nlp(query)\n",
    "query = \" \".join([tokens.text for tokens in queryDoc if not tokens.is_punct])\n",
    "\n",
    "for passage,_ in passageRanking[:10] : \n",
    "            \n",
    "    paraText = \" \".join(passage)\n",
    "    answerTuple.append(bertQA.predict(paraText, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWERS : \n",
      "('pass - throughs and collateralized mortgage obligations ( cmo )', 4.8332224)\n"
     ]
    }
   ],
   "source": [
    "print(\"ANSWERS : \")\n",
    "sortedAnswers = sorted(answerTuple, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "for item in sortedAnswers:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Timing Stemmer and Lemmatizer in both spacy and nltk ##\n",
    "\n",
    "document = \"Investors use a variety of tactics to ascertain risk. One of the most commonly used absolute risk metrics is standard deviation, a statistical measure of dispersion around a central tendency. You look at the average return of an investment and then find its average standard deviation over the same time period. Normal distributions (the familiar bell-shaped curve) dictate that the expected return of the investment is likely to be one standard deviation from the average 67% of the time and two standard deviations from the average deviation 95% of the time. This helps investors evaluate risk numerically. If they believe that they can tolerate the risk, financially and emotionally, they invest.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \" Time      .    Hey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Spacy Lemmatizer ##\n",
    "nlp = spacy.load(\"en_core_web_sm\",disable=['ner', 'parser', 'textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', '.', 'hey']\n",
      "TIME :  0.01604437828063965\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "documentDoc = nlp(document)\n",
    "nounLemma = [tokens.lemma_ for tokens in documentDoc if tokens.text.strip() != '']\n",
    "end = time()\n",
    "print(nounLemma)\n",
    "print(\"TIME : \",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertQuery(query):\n",
    "        \n",
    "        nlp = spacy.load(\"en_core_web_sm\",disable = ['ner', 'parser', 'textcat'])\n",
    "        doc = nlp(query.lower())\n",
    "        return doc\n",
    "\n",
    "def getNouns(query):\n",
    "        \n",
    "        #nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        '''\n",
    "        spacy_stopwords = list(spacy.lang.en.stop_words.STOP_WORDS)\n",
    "        \n",
    "        querySplit = self.query.lower().split()\n",
    "        \n",
    "        querySplit = [x for x in querySplit if x not in spacy_stopwords]\n",
    "        \n",
    "        print(querySplit)\n",
    "        \n",
    "        return querySplit\n",
    "        \n",
    "        '''\n",
    "        nounList = []\n",
    "        for token in query:\n",
    "            #print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" or token.pos_ == \"ADJ\":\n",
    "                nounList.append(token.text)\n",
    "            \n",
    "        #print(\"noun list : \", nounList)        \n",
    "        return nounList\n",
    "    \n",
    "def checkNounMatch(paragraph,nounList):\n",
    "        '''\n",
    "        A function to see if each noun is matching for a paragraph\n",
    "        '''\n",
    "        nounBool = True\n",
    "        \n",
    "        #nlp = spacy.load(\"en_core_web_sm\",disable=['ner', 'parser', 'textcat'])\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        #paragraphContent = \" \".join(paragraph)\n",
    "        stemmedPara = [stemmer.stem(token) for token in paragraph]\n",
    "        #paragraphDoc = nlp(paragraphContent)\n",
    "        #paragraphLemma = [tokens.lemma_ for tokens in paragraphDoc] \n",
    "        #nounDoc = nlp(\" \".join(nounList))\n",
    "        #nounsLemma = [tokens.lemma_ for tokens in nounDoc]\n",
    "        #paragraphLemma = [tokens.lemma_ for tokens in paragraphDoc if tokens.pos_ in [\"NOUN\",\"ADJ\"]]\n",
    "        for nouns in nounList: #nounsLemma:\n",
    "            \n",
    "            #nounLemma = [tokens.lemma_ for tokens in nounDoc]\n",
    "            print(stemmer.stem(nouns))\n",
    "            if stemmer.stem(nouns) in stemmedPara:#paragraphContent:\n",
    "                nounBool = True\n",
    "            else:\n",
    "                nounBool = False\n",
    "                break\n",
    "        \n",
    "        return nounBool\n",
    "                \n",
    "            \n",
    "def returnParagraphList(content,nounList):\n",
    "        '''\n",
    "        Return pargaraph in the form of list of lists to be fed to the BM25 Model\n",
    "        '''\n",
    "        paragraphList = []\n",
    "        sepCounter = 0\n",
    "        #nounList = getNouns()\n",
    "        \n",
    "        for i,term in enumerate(content):\n",
    "            \n",
    "            if term == \"--------------------------\":\n",
    "                paragraph = content[sepCounter:i] # \" \".join() <-\n",
    "                if checkNounMatch(paragraph,nounList):\n",
    "                #if all(nouns in paragraph for nouns in nounList):\n",
    "                    paragraphList.append(paragraph)\n",
    "                \n",
    "                sepCounter = i+1\n",
    "        \n",
    "        return paragraphList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the drawbacks of standard deviation?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convertQuery' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-231-a7cfbcb81681>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mqueryList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvertQuery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'convertQuery' is not defined"
     ]
    }
   ],
   "source": [
    "queryList = convertQuery(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'queryList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-232-b9c97f22aab0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mqueryList\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'queryList' is not defined"
     ]
    }
   ],
   "source": [
    "queryList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getNouns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-233-e29e6f41e27a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnounList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetNouns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueryList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'getNouns' is not defined"
     ]
    }
   ],
   "source": [
    "nounList = getNouns(queryList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['common', 'types', 'mbs']"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nounList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT :  riskmanagement.asp.txt  , TIME TAKEN :  0.04189729690551758\n",
      "DOCUMENT :  basispoint.asp.txt  , TIME TAKEN :  0.017946243286132812\n",
      "DOCUMENT :  derivative.asp.txt  , TIME TAKEN :  0.061411142349243164\n",
      "DOCUMENT :  equity.asp.txt  , TIME TAKEN :  0.051119089126586914\n",
      "DOCUMENT :  mbs.asp.txt  , TIME TAKEN :  0.025313138961791992\n",
      "DOCUMENT :  prospecttheory.asp.txt  , TIME TAKEN :  0.017072677612304688\n",
      "DOCUMENT :  balancesheet.asp.txt  , TIME TAKEN :  0.03179740905761719\n",
      "DOCUMENT :  bell-curve.asp.txt  , TIME TAKEN :  0.01796889305114746\n",
      "DOCUMENT :  benchmarkerror.asp.txt  , TIME TAKEN :  0.009974241256713867\n",
      "DOCUMENT :  calculating-covariance.asp.txt  , TIME TAKEN :  0.02587723731994629\n",
      "DOCUMENT :  certificateofdeposit.asp.txt  , TIME TAKEN :  0.1207284927368164\n",
      "DOCUMENT :  correlationcoefficient.asp.txt  , TIME TAKEN :  0.021047353744506836\n",
      "DOCUMENT :  derivative.asp.txt  , TIME TAKEN :  0.06382894515991211\n",
      "DOCUMENT :  equity.asp.txt  , TIME TAKEN :  0.05587577819824219\n",
      "DOCUMENT :  equityfinancing.asp.txt  , TIME TAKEN :  0.01795339584350586\n",
      "DOCUMENT :  expirationdate.asp.txt  , TIME TAKEN :  0.017901897430419922\n",
      "DOCUMENT :  federal-reserve-note.asp.txt  , TIME TAKEN :  0.016939163208007812\n",
      "DOCUMENT :  financial-advisor.asp.txt  , TIME TAKEN :  0.013975381851196289\n",
      "DOCUMENT :  financialasset.asp.txt  , TIME TAKEN :  0.034235477447509766\n",
      "DOCUMENT :  financialinstrument.asp.txt  , TIME TAKEN :  0.01396322250366211\n",
      "DOCUMENT :  forwardcontract.asp.txt  , TIME TAKEN :  0.012958765029907227\n",
      "DOCUMENT :  full-faith-credit.asp.txt  , TIME TAKEN :  0.015957117080688477\n",
      "DOCUMENT :  gametheory.asp.txt  , TIME TAKEN :  0.03702187538146973\n",
      "DOCUMENT :  guide-six-sigma-black-belt.asp.txt  , TIME TAKEN :  0.013966083526611328\n",
      "DOCUMENT :  how-can-derivatives-be-used-risk-management.asp.txt  , TIME TAKEN :  0.020960092544555664\n",
      "DOCUMENT :  how-standard-deviation-used-determine-risk.asp.txt  , TIME TAKEN :  0.018947839736938477\n",
      "DOCUMENT :  interestrateswap.asp.txt  , TIME TAKEN :  0.016957759857177734\n",
      "DOCUMENT :  least-squares-method.asp.txt  , TIME TAKEN :  0.012962102890014648\n",
      "DOCUMENT :  liability.asp.txt  , TIME TAKEN :  0.03590822219848633\n",
      "DOCUMENT :  market_cycles.asp.txt  , TIME TAKEN :  0.012967348098754883\n",
      "DOCUMENT :  mergersandacquisitions.asp.txt  , TIME TAKEN :  0.02692866325378418\n",
      "DOCUMENT :  mezzaninefinancing.asp.txt  , TIME TAKEN :  0.014474153518676758\n",
      "DOCUMENT :  normaldistribution.asp.txt  , TIME TAKEN :  0.01795172691345215\n",
      "DOCUMENT :  optionscontract.asp.txt  , TIME TAKEN :  0.013962745666503906\n",
      "DOCUMENT :  pipe.asp.txt  , TIME TAKEN :  0.026927471160888672\n",
      "DOCUMENT :  price_level.asp.txt  , TIME TAKEN :  0.019946575164794922\n",
      "DOCUMENT :  purerisk.asp.txt  , TIME TAKEN :  0.015957117080688477\n",
      "DOCUMENT :  random-variable.asp.txt  , TIME TAKEN :  0.016954660415649414\n",
      "DOCUMENT :  recession.asp.txt  , TIME TAKEN :  0.04262423515319824\n",
      "DOCUMENT :  regression.asp.txt  , TIME TAKEN :  0.015953540802001953\n",
      "DOCUMENT :  returnonequity.asp.txt  , TIME TAKEN :  0.044931888580322266\n",
      "DOCUMENT :  rho.asp.txt  , TIME TAKEN :  0.016004085540771484\n",
      "DOCUMENT :  risk.asp.txt  , TIME TAKEN :  0.06777453422546387\n",
      "DOCUMENT :  riskadjustedreturn.asp.txt  , TIME TAKEN :  0.018337011337280273\n",
      "DOCUMENT :  riskmanagement.asp.txt  , TIME TAKEN :  0.047870635986328125\n",
      "DOCUMENT :  sample.asp.txt  , TIME TAKEN :  0.02892327308654785\n",
      "DOCUMENT :  scenario_analysis.asp.txt  , TIME TAKEN :  0.015956640243530273\n",
      "DOCUMENT :  security.asp.txt  , TIME TAKEN :  0.05086398124694824\n",
      "DOCUMENT :  shareholdersequity.asp.txt  , TIME TAKEN :  0.0251157283782959\n",
      "DOCUMENT :  swaprate.asp.txt  , TIME TAKEN :  0.010836362838745117\n",
      "DOCUMENT :  syndicate.asp.txt  , TIME TAKEN :  0.014814615249633789\n",
      "DOCUMENT :  tranches.asp.txt  , TIME TAKEN :  0.02551722526550293\n",
      "DOCUMENT :  type-ii-error.asp.txt  , TIME TAKEN :  0.005982637405395508\n",
      "DOCUMENT :  what-does-standard-deviation-measure-portfolio.asp.txt  , TIME TAKEN :  0.009972572326660156\n"
     ]
    }
   ],
   "source": [
    "for documents in topDocuments :\n",
    "    \n",
    "    \n",
    "    start = time()\n",
    "    paragraphList = returnParagraphList(documents[1],nounList)\n",
    "    end = time()\n",
    "    print(\"DOCUMENT : \",documents[0], \" , TIME TAKEN : \",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['common', 'type', 'mbs']"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemmer.stem(x) for x in nounList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Types of Mortgage - Backed Securities . There are two common types of MBSs : pass - throughs and collateralized mortgage obligations ( CMO ) .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Types',\n",
       " 'of',\n",
       " 'Mortgage',\n",
       " '-',\n",
       " 'Backed',\n",
       " 'Securities',\n",
       " '.',\n",
       " 'There',\n",
       " 'are',\n",
       " 'two',\n",
       " 'common',\n",
       " 'types',\n",
       " 'of',\n",
       " 'MBSs',\n",
       " ':',\n",
       " 'pass',\n",
       " '-',\n",
       " 'throughs',\n",
       " 'and',\n",
       " 'collateralized',\n",
       " 'mortgage',\n",
       " 'obligations',\n",
       " '(',\n",
       " 'CMO',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common\n",
      "type\n",
      "mbs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkNounMatch([\"There\",\"are\",\"two\",\"common\",\"types\",\"of\",\"mbss\"],nounList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'secur'"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem.stem('security')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"What is implied volatility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON', 'AUX', 'VERB', 'NOUN']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokens.pos_ for tokens in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"Securities there!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [tokens.lemma_ for tokens in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['security', 'there', '!', '!', '!']"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-4574e45e7103>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not iterable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
